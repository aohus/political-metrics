import asyncio
import json
import logging
import re
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Set
from urllib.parse import parse_qs, urlparse

import aiofiles
import aiohttp


@dataclass
class BillInfo:
    """ì˜ì•ˆ ì •ë³´"""

    bill_id: str
    title: str = ""
    bill_links: List[Dict[str, str]] = field(default_factory=list)
    conf_links: List[Dict[str, str]] = field(default_factory=list)
    status: str = "pending"  # pending, processing, completed, failed
    error_msg: str = ""


@dataclass
class DownloadStats:
    """ë‹¤ìš´ë¡œë“œ í†µê³„"""

    total: int = 0
    completed: int = 0
    failed: int = 0
    skipped: int = 0
    start_time: float = field(default_factory=time.time)

    @property
    def progress_pct(self) -> float:
        return (
            (self.completed + self.failed + self.skipped) / self.total * 100
            if self.total > 0
            else 0
        )

    @property
    def elapsed_time(self) -> float:
        return time.time() - self.start_time


class MassAssemblyPDFDownloader:
    """ëŒ€ìš©ëŸ‰ êµ­íšŒ ì˜ì•ˆì •ë³´ì‹œìŠ¤í…œ ë¹„ë™ê¸° PDF ë‹¤ìš´ë¡œë”"""

    def __init__(
        self,
        max_concurrent_downloads: int = 10,
        max_concurrent_info_fetch: int = 20,
        batch_size: int = 100,
        retry_attempts: int = 3,
        delay_between_requests: float = 0.5,
    ):

        self.base_url = "https://likms.assembly.go.kr/bill/billDetail.do?billId="
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
        }

        # ë™ì‹œì„± ì œì–´
        self.max_concurrent_downloads = max_concurrent_downloads
        self.max_concurrent_info_fetch = max_concurrent_info_fetch
        self.batch_size = batch_size
        self.retry_attempts = retry_attempts
        self.delay_between_requests = delay_between_requests

        # ìƒíƒœ ê´€ë¦¬
        self.all_bill_infos: List[BillInfo] = []
        self.downloaded_files: Set[str] = set()
        self.stats = DownloadStats()
        self.logger = logging.getLogger(__name__)

        # ì„¸ì…˜ ì„¤ì •
        self.connector = aiohttp.TCPConnector(
            limit=200,
            limit_per_host=50,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        self.timeout = aiohttp.ClientTimeout(total=60, connect=10)

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‹œì‘"""
        self.session = aiohttp.ClientSession(
            headers=self.headers, timeout=self.timeout, connector=self.connector
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.session.close()
        await self.connector.close()

    def extract_bill_id_from_url(self, url: str) -> Optional[str]:
        """URLì—ì„œ billId ì¶”ì¶œ"""
        try:
            parsed = urlparse(url)
            query_params = parse_qs(parsed.query)
            return query_params.get("billId", [None])[0]
        except Exception as e:
            self.logger.error(f"URL íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    async def get_bill_info_with_retry(self, bill_id: str, bill_title: str) -> BillInfo:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì˜ì•ˆ ì •ë³´ ì¶”ì¶œ"""
        bill_url = self.base_url + bill_id

        for attempt in range(self.retry_attempts):
            try:
                return await self._get_bill_info_single(bill_url, bill_id, bill_title)
            except Exception as e:
                if attempt == self.retry_attempts - 1:
                    self.logger.error(f"ì˜ì•ˆ ì •ë³´ ì¶”ì¶œ ìµœì¢… ì‹¤íŒ¨ {bill_id}: {e}")
                    bill_info = BillInfo(
                        bill_id=bill_id,
                        title=bill_title,
                        status="failed",
                        error_msg=str(e),
                    )
                    return bill_info
                await asyncio.sleep(2**attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

    async def _get_bill_info_single(
        self, bill_url: str, bill_id: str, bill_title: str
    ) -> BillInfo:
        """ë‹¨ì¼ ì˜ì•ˆ ì •ë³´ ì¶”ì¶œ"""
        bill_info = BillInfo(bill_id=bill_id, title=bill_title, status="processing")

        async with self.session.get(bill_url) as response:
            response.raise_for_status()
            html_content = await response.text()

        bill_info.bill_links, bill_info.conf_links = self._extract_pdf_links(
            html_content
        )
        bill_info.status = "completed"
        return bill_info

    def _extract_pdf_links(
        self, html_content: str
    ) -> tuple[List[Dict[str, str]], List[Dict[str, str]]]:
        """PDF ë§í¬ ì¶”ì¶œ"""
        bill_links = []
        conf_links = []
        link_map = set()

        # JavaScript FileGate ë§í¬ íŒ¨í„´
        js_bill_pattern = r"javascript:openBillFile\(['\"]([^'\"]+)['\"],\s*['\"]([^'\"]+)['\"],\s*['\"]([^'\"]+)['\"]"
        js_matches = re.findall(js_bill_pattern, html_content, re.IGNORECASE)

        for i, match in enumerate(js_matches):
            if len(match) >= 2:
                base_url = match[0]
                book_id = match[1]
                if book_id in link_map:
                    continue

                link_map.add(book_id)
                download_url = f"{base_url}?bookId={book_id}&type=1"

                bill_links.append(
                    {
                        "url": download_url,
                        "title": "ì˜ì•ˆì›ë¬¸" if i == 0 else "ë³´ê³ ì„œ",
                        "file_id": book_id,
                        "base_url": base_url,
                    }
                )

        # íšŒì˜ë¡ ë§í¬ íŒ¨í„´
        js_conf_pattern = (
            r"javascript:openConfFile\(['\"]([^'\"]+)['\"],\s*['\"]([^'\"]+)['\"]"
        )
        js_matches = re.findall(js_conf_pattern, html_content, re.IGNORECASE)

        for match in js_matches:
            if len(match) >= 2:
                base_url = match[0]
                path = match[1]
                download_url = f"{base_url}pdf.do?confernum={path}&type=1"

                conf_links.append(
                    {
                        "url": download_url,
                        "title": "íšŒì˜ë¡",
                        "file_id": path,
                        "base_url": base_url,
                    }
                )

        return bill_links, conf_links

    async def download_pdf_with_retry(
        self, bill_info: BillInfo, output_dir: str
    ) -> Optional[Path]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ PDF ë‹¤ìš´ë¡œë“œ"""
        if not bill_info.bill_links:
            return None

        # ì˜ì•ˆì›ë¬¸ë§Œ ë‹¤ìš´ë¡œë“œ
        pdf_link = None
        for link in bill_info.bill_links:
            if link["title"] == "ì˜ì•ˆì›ë¬¸":
                pdf_link = link
                break

        if not pdf_link:
            return None

        filename = self._make_safe_filename(bill_info.title)
        file_path = Path(output_dir) / f"{filename}.pdf"

        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì²´í¬
        if file_path.exists() and file_path.stat().st_size > 1024:
            self.stats.skipped += 1
            return file_path

        for attempt in range(self.retry_attempts):
            try:
                return await self._download_pdf_single(pdf_link["url"], file_path)
            except Exception as e:
                if attempt == self.retry_attempts - 1:
                    self.logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨ {bill_info.title}: {e}")
                    self.stats.failed += 1
                    return None
                await asyncio.sleep(1 * (attempt + 1))

    async def _download_pdf_single(self, pdf_url: str, file_path: Path) -> Path:
        """ë‹¨ì¼ PDF ë‹¤ìš´ë¡œë“œ"""
        file_path.parent.mkdir(parents=True, exist_ok=True)

        async with self.session.get(pdf_url) as response:
            response.raise_for_status()

            # Content-Type ì²´í¬
            # content_type = response.headers.get("content-type", "").lower()
            # if (
            #     "pdf" not in content_type
            #     and "application/octet-stream" not in content_type
            # ):
            #     raise Exception(f"ì˜ëª»ëœ Content-Type: {content_type}")

            async with aiofiles.open(file_path, "wb") as f:
                async for chunk in response.content.iter_chunked(8192):
                    await f.write(chunk)

        # íŒŒì¼ í¬ê¸° ê²€ì¦
        # file_size = file_path.stat().st_size
        # if file_size < 1024:
        #     raise Exception("íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ (ì˜¤ë¥˜ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)")

        self.stats.completed += 1
        return file_path

    async def process_bill_batch(
        self,
        bill_batch: List[tuple],
        output_dir: str,
        info_semaphore: asyncio.Semaphore,
        download_semaphore: asyncio.Semaphore,
    ) -> List[Path]:
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬"""
        # 1ë‹¨ê³„: ì˜ì•ˆ ì •ë³´ ìˆ˜ì§‘
        info_tasks = []
        for bill_id, bill_title in bill_batch:

            async def get_info_with_semaphore(bid, btitle):
                async with info_semaphore:
                    await asyncio.sleep(self.delay_between_requests)
                    return await self.get_bill_info_with_retry(bid, btitle)

            info_tasks.append(get_info_with_semaphore(bill_id, bill_title))

        bill_infos = await asyncio.gather(*info_tasks, return_exceptions=True)
        valid_bill_infos = [
            bi
            for bi in bill_infos
            if isinstance(bi, BillInfo) and bi.status == "completed"
        ]

        # 2ë‹¨ê³„: PDF ë‹¤ìš´ë¡œë“œ
        download_tasks = []
        for bill_info in valid_bill_infos:

            async def download_with_semaphore(bi):
                async with download_semaphore:
                    await asyncio.sleep(self.delay_between_requests)
                    return await self.download_pdf_with_retry(bi, output_dir)

            download_tasks.append(download_with_semaphore(bill_info))

        results = await asyncio.gather(*download_tasks, return_exceptions=True)
        downloaded_files = [result for result in results if isinstance(result, Path)]

        # ë°°ì¹˜ë³„ bill_info ì €ì¥
        self.all_bill_infos.extend(valid_bill_infos)
        return downloaded_files

    def _log_progress(self):
        """ì§„í–‰ìƒí™© ë¡œê·¸"""
        elapsed = self.stats.elapsed_time
        rate = self.stats.completed / elapsed if elapsed > 0 else 0
        eta = (
            (
                self.stats.total
                - self.stats.completed
                - self.stats.failed
                - self.stats.skipped
            )
            / rate
            if rate > 0
            else 0
        )

        self.logger.info(
            f"ì§„í–‰: {self.stats.completed}/{self.stats.total} "
            f"({self.stats.progress_pct:.1f}%) "
            f"ì‹¤íŒ¨: {self.stats.failed}, ê±´ë„ˆëœ€: {self.stats.skipped} "
            f"ì†ë„: {rate:.1f}/sec, ETA: {eta:.0f}ì´ˆ"
        )

    def _get_new_bill_list(path: str):
        with open(path, "r") as f:
            bills = json.load(f)

        return [
            (bill["BILL_ID"], f"{bill["BILL_NO"]}_{bill["BILL_NAME"]}")
            for bill in bills
        ]

    async def download_mass_bills(
        self, path: str, output_dir: str = "./billsPDF"
    ) -> List[Path]:

        bill_list = self._get_new_bill_list(path)

        """ëŒ€ëŸ‰ ì˜ì•ˆ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ"""
        self.stats.total = len(bill_list)
        self.stats.start_time = time.time()

        self.logger.info(f"ëŒ€ëŸ‰ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {len(bill_list)}ê°œ ì˜ì•ˆ")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # ì„¸ë§ˆí¬ì–´ ìƒì„±
        info_semaphore = asyncio.Semaphore(self.max_concurrent_info_fetch)
        download_semaphore = asyncio.Semaphore(self.max_concurrent_downloads)

        all_downloaded_files = []

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(bill_list), self.batch_size):
            batch = bill_list[i : i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(bill_list) + self.batch_size - 1) // self.batch_size

            self.logger.info(
                f"ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ ì˜ì•ˆ)"
            )

            try:
                downloaded_files = await self.process_bill_batch(
                    batch, output_dir, info_semaphore, download_semaphore
                )
                all_downloaded_files.extend(downloaded_files)

                # ì§„í–‰ìƒí™© ë¡œê·¸
                if batch_num % 5 == 0:  # 5ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸
                    self._log_progress()

                # ë°°ì¹˜ ê°„ íœ´ì‹
                if i + self.batch_size < len(bill_list):
                    await asyncio.sleep(2)

            except Exception as e:
                self.logger.error(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # ìµœì¢… ê²°ê³¼
        self._log_progress()
        self.logger.info(f"ì „ì²´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(all_downloaded_files)}ê°œ íŒŒì¼")

        final_info_file = f"{output_dir}/all_bill_infos.json"
        await self.save_bill_infos(self.all_bill_infos, final_info_file)
        self.logger.info(f"ì˜ì•ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {final_info_file}")

        return all_downloaded_files

    def _make_safe_filename(self, filename: str) -> str:
        """ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±"""
        filename = re.sub(r'[<>:"/\\|?*]', "_", filename)
        filename = re.sub(r"\s+", "_", filename)
        filename = filename.strip("_")

        if len(filename) > 100:
            filename = filename[:100]

        return filename or "unknown"

    async def save_bill_infos(self, bill_infos: List[BillInfo], output_file: str):
        """ì˜ì•ˆ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        import json
        from datetime import datetime

        data = {
            "timestamp": datetime.now().isoformat(),
            "total_count": len(bill_infos),
            "bills": [
                {
                    "bill_id": bi.bill_id,
                    "title": bi.title,
                    "status": bi.status,
                    "bill_links_count": len(bi.bill_links),
                    "conf_links_count": len(bi.conf_links),
                    "bill_links": bi.bill_links,
                    "conf_links": bi.conf_links,
                    "error_msg": bi.error_msg,
                }
                for bi in bill_infos
            ],
        }

        async with aiofiles.open(output_file, "w", encoding="utf-8") as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))


async def download_mass_assembly_pdfs(
    path: str,
    output_dir: str = "./billsPDF",
    max_concurrent_downloads: int = 10,
    max_concurrent_info_fetch: int = 20,
    batch_size: int = 100,
) -> List[Path]:
    """ëŒ€ëŸ‰ PDF ë‹¤ìš´ë¡œë“œ í¸ì˜ í•¨ìˆ˜"""

    async with MassAssemblyPDFDownloader(
        max_concurrent_downloads=max_concurrent_downloads,
        max_concurrent_info_fetch=max_concurrent_info_fetch,
        batch_size=batch_size,
    ) as downloader:
        return await downloader.download_mass_bills(path, output_dir)


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("download.log", encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )

    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ì€ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
    print(f"ğŸ” ëŒ€ìš©ëŸ‰ êµ­íšŒ ì˜ì•ˆì •ë³´ì‹œìŠ¤í…œ PDF ë‹¤ìš´ë¡œë”")
    print("=" * 60)

    try:
        downloaded_files = await download_mass_assembly_pdfs(
            "path",
            output_dir="./mass_bills_pdf",
            max_concurrent_downloads=15,  # ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜
            max_concurrent_info_fetch=25,  # ë™ì‹œ ì •ë³´ ìˆ˜ì§‘ ìˆ˜
            batch_size=50,  # ë°°ì¹˜ í¬ê¸°
        )

        print(f"\nâœ… ì „ì²´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼: {len(downloaded_files)}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: ./mass_bills_pdf/")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€: pip install aiohttp aiofiles
    asyncio.run(main())
