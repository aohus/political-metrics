import gc
import json
import time
from pathlib import Path
from typing import Dict, List, Optional

import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

from .model_configs.model_registry import ModelRegistry
from .model_configs.type import GenerationConfig, ModelConfig, QuantizationConfig


class LLMBillAnalyzer:
    def __init__(self, model_name: str = "deepseek_r1_1.5b", custom_config: Optional[ModelConfig] = None):
        """
        Args:
            model_key: ModelRegistryì˜ ëª¨ë¸ í‚¤
            custom_config: ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ ì„¤ì •
        """
        if custom_config:
            self.model_config = custom_config
        else:
            self.model_config = ModelRegistry.get_model_config(model_name)
        
        self.model = None
        self.tokenizer = None
        self.generation_config = GenerationConfig(
            max_new_tokens=self.model_config.max_new_tokens,
            temperature=self.model_config.temperature
        )
        self.batch_size = 1  # ë™ì ìœ¼ë¡œ ê³„ì‚°ë¨
        
        self.setup_model()
    
    def setup_model(self):
        """ëª¨ë¸ ì„¤ì • ë° ë¡œë“œ"""
        print(f"ğŸš€ {self.model_config.display_name} ë¡œë”© ì¤‘...")
        print(f"ğŸ“ {self.model_config.description}")
        
        # ì–‘ìí™” ì„¤ì •
        quant_config = QuantizationConfig()
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=quant_config.load_in_4bit,
            bnb_4bit_compute_dtype=quant_config.bnb_4bit_compute_dtype,
            bnb_4bit_use_double_quant=quant_config.bnb_4bit_use_double_quant,
            bnb_4bit_quant_type=quant_config.bnb_4bit_quant_type,
            bnb_4bit_quant_storage=quant_config.bnb_4bit_quant_storage
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_config.name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_config.name,
            quantization_config=quantization_config,
            device_map="cuda:0",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            use_cache=False
        )
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        self._calculate_optimal_batch_size()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ’¾ ì˜ˆìƒ ë©”ëª¨ë¦¬: {self.model_config.memory_4bit:.1f}GB")
        print(f"ğŸ“¦ ìµœì  ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ğŸ·ï¸ íŠ¹í™” ë¶„ì•¼: {', '.join(self.model_config.specialties)}")
    
    def _calculate_optimal_batch_size(self):
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        try:
            memory_used = torch.cuda.memory_allocated() / 1024**3
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            memory_free = total_memory - memory_used
            
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {memory_used:.1f}GB / {total_memory:.1f}GB")
            print(f"ğŸ†“ ì—¬ìœ  ë©”ëª¨ë¦¬: {memory_free:.1f}GB")
            
            # ë°°ì¹˜ í¬ê¸° ê³„ì‚°
            if memory_free > 3:
                self.batch_size = min(self.model_config.recommended_batch_size, 8)
            elif memory_free > 2:
                self.batch_size = min(self.model_config.recommended_batch_size, 4)
            elif memory_free > 1:
                self.batch_size = min(self.model_config.recommended_batch_size, 2)
            else:
                self.batch_size = 1
                
        except Exception as e:
            print(f"âš ï¸ ë©”ëª¨ë¦¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            self.batch_size = 1
    
    def create_legal_prompt(self, json_data: Dict) -> str:
        """ë²•ì•ˆ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸"""
        title = json_data.get("title", "")
        main_content = json_data.get("sections", {}).get("ì œì•ˆì´ìœ _ë°_ì£¼ìš”ë‚´ìš©", "")
        
        # ëª¨ë¸ë³„ ìµœëŒ€ ê¸¸ì´ ì¡°ì •
        max_content_length = min(1500, self.model_config.context_length // 4)
        if len(main_content) > max_content_length:
            main_content = main_content[:max_content_length] + "..."
        
        prompt = f"""<|im_start|>system
                ë‹¹ì‹ ì€ ì •ì±… ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥ ì•ˆì„ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ì¼ê´€ëœ ë¶„ë¥˜ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.<|im_end|>
                <|im_start|>user
                ë‹¤ìŒ ë²•ë¥ ì•ˆì„ ë¶„ì„í•˜ì—¬ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

                **ë²•ë¥ ì•ˆ ì œëª©:** {title}

                **ì œì•ˆ ì´ìœ :**
                {main_content}


                ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ë¥˜ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:

                ```json
                {{
                "policy_domain_main": "ê²½ì œ|ì‚¬íšŒ|ì™¸êµêµ­ë°©|êµìœ¡|í™˜ê²½|ë³´ê±´ì˜ë£Œ|ë²•ë¬´í–‰ì •|ë†ë¦¼ìˆ˜ì‚°|êµ­í† êµí†µ|ê³¼í•™ê¸°ìˆ ",
                "policy_domain_sub": ["ì„¸ë¶€ì˜ì—­1", "ì„¸ë¶€ì˜ì—­2", "ì„¸ë¶€ì˜ì—­3"],
                "government_ministry": ["ê´€ë ¨ë¶€ì²˜1", "ê´€ë ¨ë¶€ì²˜2"],
                "target_scope": "ì „êµ­ë¯¼|íŠ¹ì •ì§€ì—­|íŠ¹ì •ì§‘ë‹¨|ê¸°ì—…|ê³µê³µê¸°ê´€",
                "target_group": ["êµ¬ì²´ì ëŒ€ìƒ1", "êµ¬ì²´ì ëŒ€ìƒ2"],
                "regulation_type": "ê·œì œê°•í™”|ê·œì œì™„í™”|ê·œì œì¤‘ë¦½|ì‹ ê·œê·œì œ",
                "regulation_intensity": 1-5,
                "market_intervention": 1-5,
                "fiscal_impact": "ì˜ˆì‚°ì¦ê°€|ì˜ˆì‚°ì¤‘ë¦½|ì˜ˆì‚°ì ˆê°|ë¯¸ìƒ",
                "policy_instrument": "ì§ì ‘ê·œì œ|ê²½ì œì ìœ ì¸|ì •ë³´ì œê³µ|ì¡°ì§ê°œí¸|ê¸°íƒ€"
                }}
                ```<|im_end|>
                <|im_start|>assistant
                ```json
                """
        return prompt
    
    def analyze_single(self, json_data: Dict) -> Dict:
        """ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„"""
        try:
            prompt = self.create_legal_prompt(json_data)
            
            # í† í°í™”
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=min(2048, self.model_config.context_length // 2),
                padding=False
            ).to(self.model.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.generation_config.max_new_tokens,
                    temperature=self.generation_config.temperature,
                    do_sample=self.generation_config.do_sample,
                    top_p=self.generation_config.top_p,
                    repetition_penalty=self.generation_config.repetition_penalty,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=False,
                    early_stopping=self.generation_config.early_stopping
                )
            
            # ê²°ê³¼ ì²˜ë¦¬
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = response[len(prompt):].strip()
            
            # JSON íŒŒì‹±
            result = self._parse_json_response(answer)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result["model_info"] = {
                "name": self.model_config.display_name,
                "specialties": self.model_config.specialties,
                "temperature": self.generation_config.temperature
            }
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del inputs, outputs
            torch.cuda.empty_cache()
            
            return result
            
        except Exception as e:
            return {"error": str(e), "model": self.model_config.display_name}
    
    def _parse_json_response(self, answer: str) -> Dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            json_start = answer.find('{')
            json_end = answer.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = answer[json_start:json_end]
                return json.loads(json_str)
            else:
                return {"error": "JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ", "raw": answer}
        except json.JSONDecodeError:
            return {"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "raw": answer}
    
    def analyze_batch(self, json_files: List[Dict], output_path: str, delay: float = 0.5) -> List[Dict]:
        """ë°°ì¹˜ ë¶„ì„"""
        print(f"ğŸ“Š {len(json_files)}ê°œ íŒŒì¼ ë¶„ì„ ì‹œì‘...")
        print(f"ğŸ¤– ëª¨ë¸: {self.model_config.display_name}")
        print(f"âš™ï¸ ì„¤ì •: T={self.generation_config.temperature}, ë°°ì¹˜={self.batch_size}")
        
        results = []
        start_time = time.time()
        
        for i, file_data in enumerate(tqdm(json_files, desc="ë¶„ì„ ì¤‘")):
            result = self.analyze_single(file_data['data'])
            result['source_file'] = file_data.get('file_path', f'file_{i}')
            results.append(result)
            
            # ì§„í–‰ ìƒí™© ë¦¬í¬íŠ¸
            if (i + 1) % 100 == 0:
                self._report_progress(results, i + 1, len(json_files), start_time)
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if i % 50 == 0:
                gc.collect()
                torch.cuda.empty_cache()
            
            if delay > 0:
                time.sleep(delay)
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(results, output_path)
        self._print_final_stats(results, start_time)
        
        return results
    
    def _report_progress(self, results: list, completed: int, total: int, start_time: float):
        """ì§„í–‰ ìƒí™© ë¦¬í¬íŠ¸"""
        elapsed = time.time() - start_time
        avg_time = elapsed / completed
        remaining = (total - completed) * avg_time
        success_rate = len([r for r in results[-100:] if "error" not in r]) if len(results) >= 100 else "ê³„ì‚°ì¤‘"
        
        print(f"â±ï¸ ì§„í–‰: {completed}/{total} ({completed/total*100:.1f}%)")
        print(f"ğŸ“ˆ í‰ê· : {avg_time:.1f}ì´ˆ/ê°œ, ë‚¨ì€ì‹œê°„: {remaining/3600:.1f}ì‹œê°„")
        print(f"âœ… ìµœê·¼ 100ê°œ ì„±ê³µë¥ : {success_rate}%")
    
    def _save_results(self, results: List[Dict], output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ì „ì²´ ê²°ê³¼
        with open(output_path / "analysis_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ì„¤ì • ì •ë³´ ì €ì¥
        config_info = {
            "model_config": {
                "name": self.model_config.name,
                "display_name": self.model_config.display_name,
                "specialties": self.model_config.specialties,
                "memory_usage": f"{self.model_config.memory_4bit:.1f}GB",
                "license": self.model_config.license
            },
            "generation_config": {
                "temperature": self.generation_config.temperature,
                "max_new_tokens": self.generation_config.max_new_tokens,
                "top_p": self.generation_config.top_p
            },
            "batch_size": self.batch_size
        }
        
        with open(output_path / "model_config.json", 'w', encoding='utf-8') as f:
            json.dump(config_info, f, ensure_ascii=False, indent=2)
    
    def _print_final_stats(self, results: List[Dict], start_time: float):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        total_time = time.time() - start_time
        success_count = len([r for r in results if "error" not in r])
        
        print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì„±ê³µ: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
        print(f"â±ï¸ ì´ ì‹œê°„: {total_time/3600:.1f}ì‹œê°„")
        print(f"âš¡ í‰ê·  ì†ë„: {total_time/len(results):.1f}ì´ˆ/ê°œ")
        print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {self.model_config.display_name}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    print("ğŸ¤– ì§€ì›ë˜ëŠ” ëª¨ë¸ë“¤:")
    for key, config in ModelRegistry.list_models().items():
        print(f"  {key}: {config.display_name} ({config.memory_4bit:.1f}GB) - {config.description}")
    
    # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ëª¨ë¸ ì¶”ì²œ
    gpu_memory = 6.0
    print(f"\nğŸ’¾ {gpu_memory}GB GPU ì¶”ì²œ ëª¨ë¸:")
    
    for priority in ["reasoning", "korean", "speed"]:
        try:
            recommended = ModelRegistry.recommend_model(gpu_memory, priority)
            print(f"  {priority}: {recommended.display_name}")
        except ValueError as e:
            print(f"  {priority}: {e}")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™” (ì¶”ë¡  ìš°ì„ )
    print(f"\nğŸš€ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
    analyzer = BaseLLMBillAnalyzer("deepseek_r1_1.5b")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = [{
        "file_path": "test.json", 
        "data": {
            "title": "í…ŒìŠ¤íŠ¸_ë²•ë¥ ì•ˆ", 
            "sections": {
                "ì œì•ˆì´ìœ _ë°_ì£¼ìš”ë‚´ìš©": "ì±„ë¬´ìì˜ ìƒê³„ë¹„ ë³´í˜¸ë¥¼ ìœ„í•œ ì••ë¥˜ê¸ˆì§€ ê³„ì¢Œ ë„ì…"
            }
        }
    }]
    
    # ë¶„ì„ ì‹¤í–‰
    results = analyzer.analyze_batch(
        test_data, 
        output_path="legal_analysis_results",
        delay=0.5
    )
    
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()


# ì‚¬ìš© ì˜ˆì‹œ ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
def test_performance(classifier, model_name):
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        "title": "2200068_ì€í–‰ë²•_ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ",
        "sections": {
            "ì œì•ˆì´ìœ _ë°_ì£¼ìš”ë‚´ìš©": "í˜„í–‰ ã€Œë¯¼ì‚¬ì§‘í–‰ë²•ã€ ì œ246ì¡°ì œ1í•­ì œ8í˜¸ì— ì˜í•˜ë©´, ì±„ë¬´ìì˜ 1ì›”ê°„ìƒê³„ìœ ì§€ì— í•„ìš”í•œ ì˜ˆê¸ˆì˜ ê²½ìš° í•´ë‹¹ ì±„ê¶Œì„ ì••ë¥˜í•˜ì§€ ëª»í•¨. ê·¸ëŸ¬ë‚˜ì‹¤ë¬´ìƒ ì••ë¥˜ ë‹¨ê³„ì—ì„œ íŠ¹ì • ì˜ˆê¸ˆì±„ê¶Œì˜ ìµœì €ìƒê³„ë¹„ ì—¬ë¶€ë¥¼ í™•ì •í•˜ê¸°ê³¤ë€í•˜ë‹¤ëŠ” ì´ìœ ë¡œ, ì¼ë‹¨ ì••ë¥˜ê°€ ì´ë£¨ì–´ì§€ê³  ê·¸ ì´í›„ í•´ë‹¹ ì˜ˆê¸ˆì±„ê¶Œì˜ìµœì €ìƒê³„ë¹„ ì—¬ë¶€ì— ê´€í•œ ë‹¤íˆ¼ì´ ì´ì–´ì§€ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì ì„. í•˜ì§€ë§Œ ì „êµ­ë¯¼ ëŒ€ë¶€ë¶„ì˜ ê²½ì œí™œë™ì´ ì˜ˆê¸ˆê³„ì¢Œë¥¼ ê¸°ì´ˆë¡œ ì´ë£¨ì–´ì§€ê¸°ë•Œë¬¸ì—, ì¼ë‹¨ ì••ë¥˜ê°€ ì´ë£¨ì–´ì§€ë©´ ê·¸ íš¨ë ¥ì´ ê³„ì†ë˜ëŠ” ë™ì•ˆ ì±„ë¬´ìì˜ì‹ ìš©ì¹´ë“œëŒ€ê¸ˆ, ì„ì°¨ë£Œ, ì „ê¸°ã†ìˆ˜ë„ã†ê°€ìŠ¤ìš”ê¸ˆ ë‚©ë¶€ ë“± ê¸°ë³¸ì  ìƒê³„ìœ ì§€ë¥¼ ìœ„í•œ í™œë™ì´ ì‚¬ì‹¤ìƒ ì–´ë ¤ì›Œì§€ê²Œ ë¨. ì´ì— ì€í–‰ìœ¼ë¡œ í•˜ì—¬ê¸ˆ ìì—°ì¸ì¸ ì±„ë¬´ìì— í•œí•˜ì—¬ 1ì¸ë‹¹ ì „ ì€í–‰ì„ í†µí‹€ì–´ 1ê°œì˜ ìƒê³„ë¹„ê³„ì¢Œë¥¼ ê°œì„¤í•  ìˆ˜ ìˆë„ë¡ í•˜ê³ , ì´ ê³„ì¢Œì— í•´ë‹¹í•˜ëŠ” ì˜ˆê¸ˆì±„ê¶Œì„ ì••ë¥˜í•˜ì§€ ëª»í•˜ë„ë¡ í•˜ë©°, í•´ë‹¹ ê³„ì¢Œì— ì••ë¥˜ê¸ˆì§€ìƒê³„ë¹„ì´ˆê³¼ ê¸ˆì•¡ì´ ì˜ˆì¹˜ë˜ë©´ ìë™ìœ¼ë¡œ ê·¸ ì´ˆê³¼ë¶„ì„ ì˜ˆë¹„ê³„ì¢Œë¡œ ì†¡ê¸ˆí•˜ë„ë¡í•˜ë„ë¡ í•˜ì—¬ ì±„ë¬´ìì˜ ìƒê³„ë¹„ë¥¼ ë³´í˜¸í•˜ë ¤ê³  í•˜ëŠ” ê²ƒì„(ì•ˆ ì œ30ì¡°ì˜3ì‹ ì„¤).",
            "ë²•ë¥ _ì œ_í˜¸": "ì œ30ì¡°ì˜3(ìƒê³„ë¹„ê³„ì¢Œ) â‘  ì€í–‰ì€ ì˜ˆê¸ˆì(ìì—°ì¸ì— í•œí•œë‹¤. ì´í•˜ ì´ì¡°ì—ì„œ ê°™ë‹¤)ì˜ ìš”ì²­ì— ë”°ë¼ ì˜ˆê¸ˆìì—ê²Œ í•„ìš”í•œ 1ì›”ê°„ì˜ ìƒê³„ë¹„ë¡œì„œ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ê¸ˆì•¡(ì´í•˜ ì´ ì¡°ì—ì„œ \"ì••ë¥˜ê¸ˆì§€ìƒê³„ë¹„\"ë¼ í•œë‹¤)ì„ ì´ˆê³¼í•˜ì—¬ ì˜ˆì¹˜í•  ìˆ˜ ì—†ëŠ” ê³„ì¢Œ(ì´í•˜ ì´ ì¡°ì—ì„œ \"ìƒê³„ë¹„ê³„ì¢Œ\"ë¼ í•œë‹¤)ë¥¼ ê°œì„¤í•  ìˆ˜ ìˆë‹¤."
        }
    }
    
    print("=" * 80)
    print("6GB GPU ìµœì í™” ì •ì±…ë¶„ë¥˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
    classifier = classifier(model_name)
    
    # ë¶„ë¥˜ ìˆ˜í–‰
    print("\në¶„ë¥˜ ìˆ˜í–‰ ì¤‘...")
    result = classifier.classify_fast(test_data)
    
    print("\n" + "=" * 80)
    print("ë¶„ë¥˜ ê²°ê³¼:")
    print("=" * 80)
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    return result



if __name__ == "__main__":
#     # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    classifier = LLMBillAnalyzer
    model_name = "model_name"
    result = test_performance(classifier, model_name)
    
#     # ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì‹œ
#     batch_data = [test_data] * 5  # 5ê°œ ë¬¸ì„œ í…ŒìŠ¤íŠ¸
#     batch_results = classifier.batch_classify(batch_data)